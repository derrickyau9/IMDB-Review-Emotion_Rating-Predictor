{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4obK4KQj851S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd9233e-c47d-4f8f-fc24-a1d5a782ae8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from imblearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Attempt to read with UTF-8 encoding\n",
        "    df = pd.read_csv('https://raw.githubusercontent.com/derrickyau9/IMDB-Review-Emotion_Rating-Predictor/main/CSVs/Merged_Reviews_Final.csv', encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    # If UTF-8 fails, try reading with ISO-8859-1 encoding\n",
        "    df = pd.read_csv('https://raw.githubusercontent.com/derrickyau9/IMDB-Review-Emotion_Rating-Predictor/main/CSVs/Merged_Reviews_Final.csv', encoding='ISO-8859-1')\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Joining the tokens back into text\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "NcmoGfQUCV0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing to the review column\n",
        "df['Review'] = df['Review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "w-h7ErIBCZ_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "XOZJ85scEayE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target labels\n",
        "X = df['Review']\n",
        "y = df.drop(['Review'], axis=1)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Feature extraction with TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "P13U0Qp3Cnme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train_tfidf.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(y_train.shape[1], activation='sigmoid'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_tfidf_dense, y_train, epochs=20, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "GBF2OCwaE21X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f4757f-f16f-42a2-8b92-dc0792a9f3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "29/29 [==============================] - 7s 13ms/step - loss: 0.6752 - accuracy: 0.2633 - val_loss: 0.6363 - val_accuracy: 0.3378\n",
            "Epoch 2/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.5863 - accuracy: 0.2878 - val_loss: 0.5429 - val_accuracy: 0.3378\n",
            "Epoch 3/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.5263 - accuracy: 0.2611 - val_loss: 0.5294 - val_accuracy: 0.4044\n",
            "Epoch 4/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.4827 - accuracy: 0.3411 - val_loss: 0.5080 - val_accuracy: 0.4133\n",
            "Epoch 5/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.3800 - val_loss: 0.4784 - val_accuracy: 0.4044\n",
            "Epoch 6/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.3545 - accuracy: 0.4233 - val_loss: 0.4605 - val_accuracy: 0.3867\n",
            "Epoch 7/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.4300 - val_loss: 0.4594 - val_accuracy: 0.3956\n",
            "Epoch 8/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2893 - accuracy: 0.4222 - val_loss: 0.4636 - val_accuracy: 0.3956\n",
            "Epoch 9/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2692 - accuracy: 0.4500 - val_loss: 0.4646 - val_accuracy: 0.3600\n",
            "Epoch 10/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2511 - accuracy: 0.4333 - val_loss: 0.4693 - val_accuracy: 0.3556\n",
            "Epoch 11/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2285 - accuracy: 0.4367 - val_loss: 0.4680 - val_accuracy: 0.3733\n",
            "Epoch 12/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2133 - accuracy: 0.4233 - val_loss: 0.4777 - val_accuracy: 0.3689\n",
            "Epoch 13/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.4489 - val_loss: 0.4841 - val_accuracy: 0.3822\n",
            "Epoch 14/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1842 - accuracy: 0.4167 - val_loss: 0.4947 - val_accuracy: 0.3556\n",
            "Epoch 15/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1682 - accuracy: 0.4456 - val_loss: 0.4979 - val_accuracy: 0.3378\n",
            "Epoch 16/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.4267 - val_loss: 0.4960 - val_accuracy: 0.3422\n",
            "Epoch 17/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1459 - accuracy: 0.4356 - val_loss: 0.5238 - val_accuracy: 0.3467\n",
            "Epoch 18/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.4356 - val_loss: 0.5211 - val_accuracy: 0.3511\n",
            "Epoch 19/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.4378 - val_loss: 0.5329 - val_accuracy: 0.3422\n",
            "Epoch 20/20\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1229 - accuracy: 0.4267 - val_loss: 0.5506 - val_accuracy: 0.3289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d612b9f1720>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Attempt to read with UTF-8 encoding\n",
        "    full_df = pd.read_csv('https://raw.githubusercontent.com/derrickyau9/IMDB-Review-Emotion_Rating-Predictor/main/CSVs/50k_Full_Data.csv', encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    # If UTF-8 fails, try reading with ISO-8859-1 encoding\n",
        "    full_df = pd.read_csv('https://raw.githubusercontent.com/derrickyau9/IMDB-Review-Emotion_Rating-Predictor/main/CSVs/50k_Full_Data.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Apply the same preprocessing to the full dataset\n",
        "full_df['review'] = full_df['review'].apply(preprocess_text)\n",
        "\n",
        "# Separate features\n",
        "X_full = full_df['review']\n",
        "\n",
        "# Transform features using the same TF-IDF Vectorizer\n",
        "X_full_tfidf = vectorizer.transform(X_full)\n",
        "\n",
        "# Convert to dense format if required by the model\n",
        "X_full_tfidf_dense = X_full_tfidf.toarray()\n",
        "\n",
        "# Use the trained model to make predictions\n",
        "predictions = model.predict(X_full_tfidf_dense)"
      ],
      "metadata": {
        "id": "cLf_UcKeFFMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db0b76c-5c72-456f-9c21-0e40a795b967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.25\n",
        "binary_predictions = (predictions > threshold).astype(int)"
      ],
      "metadata": {
        "id": "Da3BNfttFGwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'label_columns' contains the names of your label columns\n",
        "label_columns = ['Disgust', 'Disappointment', 'Sadness', 'Confusion', 'Anger', 'Fear', 'Indifference', \\\n",
        "                 'Surprise', 'Interest', 'Happiness', 'Reflective']\n",
        "\n",
        "# Convert binary predictions to a DataFrame\n",
        "predictions_df = pd.DataFrame(binary_predictions, columns=label_columns)\n",
        "\n",
        "# Concatenate the predictions with the full DataFrame\n",
        "full_df_with_predictions = pd.concat([full_df, predictions_df], axis=1)"
      ],
      "metadata": {
        "id": "B-QsMlxXFMjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df_with_predictions.to_csv('/content/50k_full_data_with_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "HtlWJytIFQjY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}